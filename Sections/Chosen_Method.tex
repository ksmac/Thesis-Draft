Machine learning (ML) is an increasingly popular data analytic technique widely employed not limited to particle physics. It is useful for scenarios where a large amount of data generated or observed requires analyses that cannot simply be done by traditional statistical techniques. In high energy physics, the use of machine learning has made a significant impact especially in the discovery of the Higgs boson using Boosted Decision Trees \cite{chatrchyan2012observation, aad2012observation, chen2015higgs}. However, machine learning is not an all-mighty tool that could discover new physics. Applying existing conditions to searches reduces unwanted information, resulting in the algorithm working harder but allowing it to optimize its search for new physics. This process is known as \textit{pre-selection}, and it is a crucial step in our method. \\


%-------------------------------------------------------------------------%
\section{Machine learning}
Machine learning is an umbrella term for algorithms that ``learns" patterns from a given data set whether small or large to give predictions. There are two general categories in ML, which are known as \textit{supervised learning} and \textit{unsupervised learning}. Supervised learning requires a separate data with labeled outcomes (simulations) to build an effective model for analyzing the raw data whose outcomes are not labeled. This process is known as \textit{training}. Unsupervised techniques, on the other hand, do not require training as they are purely data-driven methods opposed to relying on simulations like supervised techniques. Unbalanced and difficult datasets produced in collider experiments are most effective with supervised techniques, and so ML referred to hereon will imply supervised techniques. \\

Training a dataset is relatively simple. The process involves importing the data onto the chosen platform, choosing which algorithm to use and follow its syntax in building a model, including which parameters to use. Most existing open-source algorithms are extremely versatile and already yield a high performance, which allows for easy use. Models are built by minimizing a chosen error, where common choises are simple metrics such as \textit{Mean Squared Error (MSE)}, \textit{Residual MSE (RMSE)}, \textit{Log-loss} etc. However, when training a classifier one must be careful of \textit{overtraining}, a relatively common error that many encounter. Overtraining occurs when a model's parameters are too restrictive, limiting a model to be versatile in its performance. Versitility here implies that how a model performs under various datasets that are similar in nature. In other words, by restricting a model to perform well under a single dataset, we limit ourselves in performing well when it counts i.e. the raw data. \\

Overtraining is not a difficult mistake to avoid. Cross-validations (CV) is an effective method to prevent overtraining. CV is a task that requires the training data to be further partitioned to create a smaller set of training and test data. The most common CV is the \textit{k-fold CV}, where the training data is randomly sampled into $k$ different subsets. By selecting $k-1$ of these subsets as the training data to build the classifier on, the remaining subset is used to validate this model on. This procedure is repeated $k$ times through each split, where each performance is scored. The overall result typically involves taking the average of the $k$-fold scores.  There are no set rules on what the best value of $k$ is, however, most ML users prefer to stick to either 5 or 10 due to its simplicity and performance yield \cite{james2013introduction}. \\

Upon building a classifier, it is difficult to find the most optimal parameters whilst being cautious of overtraining. In order to overcome this dilemma, a grid search can be performed. A grid search utilizes CV at its foundations to test the performance of the selected parameters. By utilizing this feature, it is possible to check through a range of parameters. The down-side to this, however, is that it is computationally expensive and multiple evaluations may be required to even find some consistency in the results before narrowing down some parameters. \\

Fine-tuning parameters is not the only effective method to improve the classifier. A common technique known as \textit{feature engineering} can be employed to manipulate the existing variables so that the prediction accuracy improves across the models built, although not guaranteed. In this work, a simple feature engineering was performed. This entailed of rearranging the four jet entries such that the highest $p_T$ jet with a b-tag is considered the b-jet originating from the decay, and the subsequent jets are ordered by the $p_T$ values regardless of whether there is a b-tag or not. \\

The most popular algorithms employed by the scientific community (not limited to physics) are Neural Networks (NNs) and Decision Trees (DTs). In the correct setting, either methods perform exceptionally well. The trend in many communities favour NNs over DTs. However, through some preliminary tests using the \textit{h2o} package's \cite{h2o} \textit{automl} function, it was shown that a tree-based method known as \textit{Extreme Gradient Boosting (EGB)} is most suited for this task. The package that contains this feature is known as \textit{xgboost} , both available as a standalone use \cite{xgboost} or an extension of the h2o package \cite{h2o}. \\

%-------------------------------------------------------------------------%
\section{Tree-based methods and Extreme Gradient Boosting}
\label{sec:method}
Tree-based methods can intuitively be thought of as an extension to a cut-flow analysis\footnote{Cut-flow analysis is a traditional technique employed to reduce background events according to various physically motivated criteria. The final result gives a maximized Signal-to-Noise ratio.}. Instead of discarding selections that do not fulfill a certain criteria, these selections may be further explored upon by the algorithm provided that a new criteria exists. However, there is no known method to extract any `physical' criteria from such splits, as these splits are performed by minimizing a chosen loss-function. A simple diagram is depicted in Figure \ref{fig:tree}, where the mathematical form of a regression tree is described in Equation \ref{eq:DT}. Here, $c_m$ is the coefficient associated to the partition of the data, $R_m$ \cite{james2013introduction}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=10cm, height= 6cm]{DT.png}
    \caption{A simple decision tree for a hypothetical signal selection such that an extra parameter $g(x,y)$ is present within the algorithm.}
    \label{fig:tree}
\end{figure}

\begin{equation}
    f(x) = \sum_{i=1}^m c_m I(x\in R_m)
    \label{eq:DT}
\end{equation}

There are many tree-based methods that vary in the data sampling method and how a loss-function is minimized. In the case of EGB models, it is an extension to the \textit{Gradient Boosted Machine (GBM)} developed by Jerome Friedman \cite{friedman2001greedy}. \textit{Boosting} is an iterative algorithm in which an ensemble of weak models\footnote{A weak model consists of a small decision tree that is not an effective approximation to the data of interest.} are built sequentially, correcting its previous model through re-weighting. This leads to a final model that is highly representative of our data. Gradient boosting, extends this idea such that a differentialble loss-function's gradient (known as gradient descent) allows the errors to reduce locally, until the error is as close to zero as possible. EGB/XGBoost adds to the features of regular GBMs with weighted quantile splittings and its ability to manage sparse\footnote{Since there are no missing entries in this project, the data is not sparse, or rather, it is `dense'.} data, incorporating parallel computing to achieve faster computation time compared to regular GBM algorithms \cite{chen2016xgboost}. Regularization, a method to avoid overfitting, is also available by adding penalty terms to the loss-fucntion, making its predictive power more reliable. \\

%-------------------------------------------------------------------------%
\section{Metrics for model performance}
\label{sec:metrics}
The performance of a classifier can be boiled down a single table known as a confusion matrix. Displayed in Table \ref{tab:ConfMat}, a confusion matrix visually shows the distribution of correctly and incorrectly classified points within the dataset. The correctly classified signal and background events are the True Positives (TP) and True Negatives (TN), respectively. Likewise, the incorrectly classified signal and background events are the False Negatives (FN) and False Positive (FP), respectively. The accuracy of the classifier is given by TP+TN/N, and the higher the accuracy, the higher the SBR. \\

% Confusion matrix template
\begin{table}[htbp]
    \centering 
    \begin{tabu}{c|[2pt]c|c|c|c}
        \multicolumn{2}{c}{}&\multicolumn{2}{c}{Predicted}&\\
        \tabucline[2pt]{3-5}
        \multicolumn{2}{c|[2pt]}{}
        &\multicolumn{1}{c|}{Background} &\multicolumn{1}{c|[2pt]}{Signal} &\multicolumn{1}{c|[2pt]}{Total}\\
        \tabucline[2pt]{2-5}
        \multirow{\items}{*}{\rotatebox{90}{Simulated}}
        &\multicolumn{1}{c|[2pt]}{Background} & \multicolumn{1}{c|}{TN} & \multicolumn{1}{c|[2pt]}{FP} & \multicolumn{1}{c|[2pt]}{TN$+$FP} \\
        \cline{2-5}
        \multicolumn{1}{c|[2pt]}{}& \multicolumn{1}{c|[2pt]}{Signal} & \multicolumn{1}{c|}{FN} & \multicolumn{1}{c|[2pt]}{TP} & \multicolumn{1}{c|[2pt]}{FN$+$TP} \\
        \tabucline[2pt]{2-5}
        \multicolumn{1}{c|[2pt]}{} & \multicolumn{1}{c|[2pt]}{Total} & \multicolumn{1}{c|}{TN$+$FN} & \multicolumn{1}{c|[2pt]}{FP$+$TP} & \multicolumn{1}{c|[2pt]}{N}\\
        \tabucline[2pt]{2-5}
    \end{tabu}
    \caption{A confusion matrix for truth (simulated) and predicted labels and its components. The diagonal components are the correctly classified background (TN) and signal (TP) events. The off-diagonal components are the mis-classified background (FP) and signal (FN) events. }
    \label{tab:ConfMat}
\end{table}

An evaluation metric presented by the Higgs Challenge \cite{adam-bourdarios_learning_2014} is the \textit{approximate median significance (AMS)}, given by Equation \ref{eq:AMS}
\begin{equation}
    \text{AMS} = \sqrt{2\Big((s+b+b_r)\ln\Big(1+\frac{s}{b+b_r}\Big)\Big)}
    \label{eq:AMS}
\end{equation}
where $s$ and $b$ are the unnormalized TP and FP rates, respectively, and $b_r$ is a constant regularization term set at 10. This measure is a derivation of the significance Z given by Equation (\ref{eq:Z})
\begin{equation}
    \text{Z} = \sqrt{2\Big( n\ln\Big(\frac{n}{\mu_b}\Big)-n+\mu_b\Big)}
    \label{eq:Z}
\end{equation}
where $n$ is the number of events and $\mu_b$ is the mean of the background, requiring that $n>\mu_b$. The values $n$ and $\mu_b$ are replaced by $s+b$ and $b$ in Equation (\ref{eq:AMS}), respectively. Traditionally, the Z significane at $Z=5$ corresponds to a $p$-value of less than $2.9\times10^{-7}$, sufficient to claim a discovery \cite{adam-bourdarios_learning_2014}. However, the regularization term $b_r$ is equal to zero in this setting, thus differing this significance of values given by either equations. \\

In order to obtain $s$ and $b$, we follow Equation \ref{eq:N}
\begin{equation}
    s(b) = N_{s(b)}\times \epsilon_{s(b)} 
    \label{eq:N}
\end{equation}
where $N_{s(b)} = \sigma \int L(t) dt = \sigma \times 137$ \cite{thomson2013modern} is the number of expected events, and $\epsilon=\frac{\text{FP}}{\text{N}}$ is the efficiency of the classifier given a dataset of size N, where in our case, would be a third of each dataset partitioned as our test set.  
%-------------------------------------------------------------------------%
%\section{}


%-------------------------------------------------------------------------%